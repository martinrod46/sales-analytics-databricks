# ğŸ“Š Sales Analytics Pipeline (Databricks + PySpark + SQL)

### ğŸš€ Project Overview

This project is an end-to-end sales analytics pipeline designed to simulate a real-world data analyst / analytics engineer workflow using Databricks, PySpark, and SQL.

The pipeline ingests raw CSV sales data, stages it using SQL, transforms it with PySpark, and produces analytics-ready dimension and fact tables that can be queried for business insights.

The goal of this project is to demonstrate:

- Data modeling 
- ETL / ELT concepts 
- SQL analytics 
- PySpark transformations 
- Git-based workflow 
- Databricks + Unity Catalog best practices


<br>
<br>


### ğŸ§± Architecture

Raw CSV Files \
   â†“ \
Unity Catalog Volume (raw_sales_data) \
   â†“ \
Staging Tables (SQL) \
   â†“ \
PySpark ETL \
   â†“ \
Analytics Tables (dim & fact) \
   â†“ \
Business Analytics (SQL queries) 

<br>
<br>

### ğŸ› ï¸ Tech Stack

- Databricks -> Execution environment 
- Apache Spark (PySpark) -> ETL transformations 
- SQL -> Staging, modeling and analytics 
- Unity Catalog -> Data governance and storage 
- GitHub -> Version control

<br>
<br>

### ğŸ“ Repository Structure

sales_analytics_pipeline/ \
â”œâ”€â”€ pyspark/ \
â”‚   â””â”€â”€ etl_sales_analytics.py \
â”‚ \
â”œâ”€â”€ sql/ \
â”‚   â””â”€â”€ analytics_queries.sql \
â”‚ \
â””â”€â”€ README.md 

<br>
<br>

### ğŸ“¥ Data Ingestion

Raw data is stored as CSV files in a Unity Catalog Volume.

Files include: 

- Customers 
- Products 
- Orders 
- Order_items 

**Data is loaded into staging tables using SQL.**

<br>
<br>

### ğŸ”„ ETL Process (PySpark)

The PySpark ETL script performs the following:

Dimensions:

- dim_customers 
- dim_products 

Fact Table:

- fact_sales

joins: 
- orders 
- order_items
- products

calculates revenue per line item

<br>
<br>

### ğŸ“Š Analytics & Business Queries

The SQL layer answers common business questions such as:

- Revenue by day 
- Revenue by product category 
- Top customers by spend 
- Average order value 


 These queries are stored in:

>> ### sql/analytics_queries.sql

<br>
<br>

### â–¶ï¸ How to Run

- Upload raw CSV files to a Unity Catalog volume. 
- Create staging tables using SQL. 
- Query analytics tables using SQL.

 Run the PySpark ETL script:

 >> ### pyspark/etl_sales_analytics.py



<br>
<br>

### ğŸ“Œ Notes

- Raw data is not stored in GitHub. 
- All processing is done inside Databricks. 
- Project follows industry-standard analytics workflows.

<br>
<br>

### ğŸ‘¤ Author

Built as a hands-on learning project to practice real-world data analytics and engineering skills.

<br>
<br>

### ğŸ’¼ Job Interview Insights

Built an interview-proof guide that can help in case you need to talk through this project confidently.

>> ### interview_insights.md document 